{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec81ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d4637",
   "metadata": {},
   "source": [
    "# Нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d996a0",
   "metadata": {},
   "source": [
    "Чтобы разобраться в нейронных сетях и глубоком обучении, нужно идети от простого перцепторна, к нейронным сетям. Посмотрим на перцептрон:\n",
    "\n",
    " <img src=\"./images/perceptron.png\" alt=\"Тензор\" title=\"Тензор\" width=\"300\" />\n",
    "\n",
    "Что здесь происходит? Слева находятся входы ($X_1, X_2, X_3$). В контексте анализа данных о книгах это могут быть количество страниц, год издания или цена.\n",
    "\n",
    "Каждый вход имеет свой вес ($W_1, W_2, W_3$). Вес определяет «важность» конкретного признака для нейрона.\n",
    "\n",
    "Сверху подведено смещение ($w_0$ или $b$). Оно позволяет нейрону гибко настраивать порог срабатывания, даже если все входные иксы равны нулю.\n",
    "\n",
    "В центре нейрона происходит линейная операция — вычисление взвешенной суммы:\n",
    "\n",
    "$$z = w_0 + \\sum_{j=1}^{p} w_{j} X_j$$\n",
    "\n",
    "Это «сырой» результат работы нейрона. На схеме он обозначен как Threshold Sum. Здесь признаки объединяются в одно число, которое еще не является финальным ответом.\n",
    "\n",
    "Дальше идет Функция активации (Step Threshold). Это «пороговый» механизм. В оригинальном перцептроне использовалась жесткая ступенчатая функция. Если сумма $z$ больше нуля, на выходе получаем 1 (нейрон «активен»). Если сумма меньше или равна нулю, на выходе 0.\n",
    "\n",
    "Внизу показан процесс обучения. Спрогнозированный выход сравнивается с реальным значением ($y$).\n",
    "\n",
    "Разница между ними — это Ошибка (Error).\n",
    "\n",
    "На основании этой ошибки происходит Обновление весов ($\\Delta W$). Алгоритм корректирует $W$ так, чтобы в следующий раз при таких же входах ошибка стала меньше."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e34067f",
   "metadata": {},
   "source": [
    "Теперь эту абстрактную модель перцептрона нужно контрезировать так: \n",
    "\n",
    "$$f(X) = \\beta_0 + \\sum_{k=1}^{K} \\beta_k g \\left( w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right)$$\n",
    "\n",
    "Одиночный нейрон в этой формуле вот:\n",
    "\n",
    "$$g \\left( w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right)$$\n",
    "\n",
    "Иксы - переменные, каждая из которых умножается на свой вес, потом происходит суммирование, результат которого переходит в функцию активации. Таких нейронов может быть много. Если их собрать в сеть, то мы как раз получим:\n",
    "\n",
    "$$\\sum_{k=1}^{K} g \\left( w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right)$$\n",
    "\n",
    "А вот это уже можно поместить в линейную форму и начать обучаться, то есть как раз перейти к полной форме для всех нейронов:\n",
    "\n",
    "$$f(X) = \\beta_0 + \\sum_{k=1}^{K} \\beta_k g \\left( w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right)$$\n",
    "\n",
    "Это можно для удобства переписать и так:\n",
    "\n",
    "Мы уже видели, что методы машинного обучения крутятся вокруг суммы. Нейронные сети от этого уходят недалеко:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n",
    "&= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g \\left( w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Как образовалась эта формула мы видели. Здесь $h_k(X)$ — скрытый нейрон (Hidden Unit).\n",
    "\n",
    "Если бы мы не трансформировали $h_k(X)$, то у нас получилась бы обычная линейная регрессия. Однако мы применяем какую-то нелиненую трансформацию сигмоида $\\sigma(z)$ или ReLU $(z)$. Итак, еще раз посмотрим на это:\n",
    "\n",
    "$$ g \\left( w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right) $$\n",
    "\n",
    "В скобках \"сырой\" нейрон. Результат, который мы получим в скобках, мы пропустим через функцию g, которая как раз и может быть, например, ReLU.\n",
    "\n",
    "Итак, здесь $w_{k0}$ — смещение нейрона (Neuron Bias). Это можно считать \"пороговой\" функцией, тем значением, с которого нейрон \"срабатывает\". \n",
    "\n",
    "$\\sum_{j=1}^{p}$ — сумматор входов. Входами считаются здесь перменные, то есть x. \n",
    "\n",
    "$w_{kj}$ — входные веса (Input Weights). Параметры матрицы $W$. Они показывают, как сильно $j$-й признак (например, num_pages) влияет на активацию $k$-го нейрона. $X_j$ — Входной признак. Конкретное значение $j$-го признака (например, \"350 страниц\").\n",
    "\n",
    "Вся формула описывает двухэтапный процесс. Внутри ($w, X$) мы берем признаки $X$, взвешиваем их весами $w$ и пропускаем через нелинейность $g$. Получаем новые, \"умные\" признаки $h(X)$.Снаружи ($\\beta, h$): мы берем эти новые признаки $h(X)$ и строим над ними обычную линейную комбинацию с весами $\\beta$.\n",
    "\n",
    "Отличие от обычной линейной модели в том, что у нас бета умножается не просто на исходно данное X, а на некое преобразованное значение, построенное на основе X. Как мы уже говорили, машинное обучение не может вырваться за пределы суммирования, а значит ему нужно как-то изменять данное. Нейронная сеть и есть такой способ изменения. Функция g здесь становится основой всего. Популярной является функция ReLU:\n",
    "\n",
    "$$\n",
    "g(z) = (z)_+ = \\begin{cases} \n",
    "0, & \\text{если } z < 0 \\\\ \n",
    "z & \\text{в противном случае} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Предположим, что мы внтури g получили результат -5.4. Подставляем в ReLU. Так как $-5.4 \\le 0$, то $g(-5.4) = 0$. \n",
    "\n",
    "Давайте посмотрим, как работает эта наша формула.\n",
    "\n",
    "Берем уже известную формулу:\n",
    "$$f(X) = \\beta_0 + \\sum_{k=1}^{K} \\beta_k g \\left( w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right)$$\n",
    "\n",
    "Нам дано $K=2$ нейрона, $p=2$ входа. Кроме того, мы знаем веса и беты:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Нейрон 1:} \\quad & w_{10} = 0, & w_{11} = 1, & w_{12} = 1 \\\\\n",
    "\\text{Нейрон 2:} \\quad & w_{20} = 0, & w_{21} = 1, & w_{22} = -1\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\\beta_0 = 0, \\quad \\beta_1 = \\frac{1}{4}, \\quad \\beta_2 = -\\frac{1}{4}$$\n",
    "\n",
    "Вычисляем скрытый слой $h_k$.\n",
    "\n",
    "Сначала формируется линейная комбинация входов $z$, затем применяется функция активации $g(z) = z^2$.\n",
    "\n",
    "#### Нейрон 1 (Сумма)\n",
    "Вычисляем взвешенную сумму:\n",
    "$$z_1 = w_{10} + w_{11}X_1 + w_{12}X_2 = 0 + 1\\cdot X_1 + 1\\cdot X_2 = X_1 + X_2$$\n",
    "Применяем квадратичную активацию:\n",
    "$$h_1(X) = (z_1)^2 = (X_1 + X_2)^2$$\n",
    "\n",
    "#### Нейрон 2 (Разность)\n",
    "Вычисляем взвешенную сумму:\n",
    "$$z_2 = w_{20} + w_{21}X_1 + w_{22}X_2 = 0 + 1\\cdot X_1 + (-1)\\cdot X_2 = X_1 - X_2$$\n",
    "Применяем квадратичную активацию:\n",
    "$$h_2(X) = (z_2)^2 = (X_1 - X_2)^2$$\n",
    "\n",
    "Теперь вычисляем выходной слой ($f(X)$). Подставляем полученные значения $h_1$ и $h_2$ в линейное уравнение выходного слоя:\n",
    "\n",
    "$$f(X) = \\beta_0 + \\beta_1 h_1(X) + \\beta_2 h_2(X)$$\n",
    "\n",
    "Делаем подстановку числовых значений весов $\\beta$:\n",
    "$$f(X) = 0 + \\frac{1}{4}(X_1 + X_2)^2 + \\left(-\\frac{1}{4}\\right)(X_1 - X_2)^2$$\n",
    "\n",
    "Выносим общий множитель $\\frac{1}{4}$ за скобки:\n",
    "$$f(X) = \\frac{1}{4} \\left[ (X_1 + X_2)^2 - (X_1 - X_2)^2 \\right]$$\n",
    "\n",
    "Чтобы убедиться, что результатом действительно является умножение, раскроем скобки по формулам сокращенного умножения:\n",
    "1.  Квадрат суммы: $(a+b)^2 = a^2 + 2ab + b^2$\n",
    "2.  Квадрат разности: $(a-b)^2 = a^2 - 2ab + b^2$\n",
    "\n",
    "Подставляем эти раскрытия в наше уравнение:\n",
    "$$f(X) = \\frac{1}{4} \\left[ (X_1^2 + 2X_1X_2 + X_2^2) - (X_1^2 - 2X_1X_2 + X_2^2) \\right]$$\n",
    "\n",
    "Раскрываем внутренние скобки, меняя знаки у вычитаемого выражения:\n",
    "$$f(X) = \\frac{1}{4} \\left[ X_1^2 + 2X_1X_2 + X_2^2 - X_1^2 + 2X_1X_2 - X_2^2 \\right]$$\n",
    "\n",
    "Группируем подобные члены. Квадраты переменных с противоположными знаками взаимно уничтожаются:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X_1^2 - X_1^2 &= 0 \\\\\n",
    "X_2^2 - X_2^2 &= 0 \\\\\n",
    "2X_1X_2 + 2X_1X_2 &= 4X_1X_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "В результате в скобках остается только удвоенное перекрестное произведение:\n",
    "$$f(X) = \\frac{1}{4} [ 4 X_1 X_2 ]$$\n",
    "\n",
    "Сокращаем четверки:\n",
    "$$f(X) = X_1 X_2$$\n",
    "\n",
    "Вот как это можно представить графически. Нужно помнить, что $$ A_k = h_k(X) = g \\left( w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right), $$\n",
    "\n",
    "<img src=\"./images/neur.png\" alt=\"Тензор\" title=\"Тензор\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a5363",
   "metadata": {},
   "source": [
    "У нас есть формула, которая собирает результаты срабатывания нейронов. Где же здесь обучение? Обучением считается изменение весов. Не сами веса, а именно возможность их изменения. Мы изменяем веса таким образом, чтобы наша формула давала нам правдивые зависимые переменные. Давайте посмотрим, какая математика за этим. \n",
    "\n",
    "Сначала нам надо определить функцию потерь, то есть такую функцию, которая устанавливает, насколько предсказанное значение хуже реального. Это и есть математический анализ идеи \"обучить так, чтобы предсказывала лучше\":\n",
    "\n",
    "$$E = \\frac{1}{2} (y - f(X))^2$$\n",
    "\n",
    "$f(X)$, если посмотреть чуть выше, зависит от бет и иксов. Наша цель — минимизировать $E$, значит нужна производная от $f(X)$, но эту производную нельзя посчитать без производной по бете, потому что изменение $f(X)$ зависит от изменения беты.\n",
    "\n",
    "В итоге имеем:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\beta_k} = \\frac{\\partial E}{\\partial f} \\cdot \\frac{\\partial f}{\\partial \\beta_k}$$\n",
    "\n",
    "Мы определили функцию потерь как половину квадрата разности между истиной ($y$) и предсказанием ($f$):\n",
    "\n",
    "$$E = \\frac{1}{2}(y - f)^2$$\n",
    "\n",
    "Чтобы найти производную по $f$, мы используем правило дифференцирования сложной функции: $(u^n)' = n \\cdot u^{n-1} \\cdot u'$.\n",
    "Пусть $u = (y - f)$. Тогда:\n",
    "\n",
    "Внешняя производная (степенная): $2 \\cdot \\frac{1}{2} \\cdot (y - f)^{2-1} = (y - f)$.\n",
    "\n",
    "Внутренняя производная (по переменной $f$): $\\frac{\\partial}{\\partial f}(y - f) = -1$ (так как $y$ для нас — константа, а производная $-f$ равна $-1$).\n",
    "\n",
    "Перемножаем: $(y - f) \\cdot (-1) = -(y - f)$.\n",
    "\n",
    "Знак минуса здесь критически важен: он разворачивает направление градиента в сторону уменьшения ошибки.\n",
    "\n",
    "Теперь ищем $\\frac{\\partial f}{\\partial \\beta_k}$.\n",
    "\n",
    "Мы ищем частную производную $\\frac{\\partial f}{\\partial \\beta_k}$. Это значит, что все остальные слагаемые мы считаем константами:\n",
    "\n",
    "$\\beta_0$ — константа, производная равна $0$;\n",
    "\n",
    "$\\beta_i h_i(X)$ (где $i \\neq k$) — константы, их производные равны $0$;\n",
    "\n",
    "Остается только слагаемое $\\beta_k h_k(X)$.\n",
    "\n",
    "Так как $h_k(X)$ в данном контексте — это просто коэффициент при переменной $\\beta_k$, производная берется как $(c \\cdot x)' = c$:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\beta_k} (\\beta_k h_k(X)) = h_k(X)$$\n",
    "\n",
    "Можно переписать $$\\frac{\\partial E}{\\partial \\beta_k} = \\frac{\\partial E}{\\partial f} \\cdot \\frac{\\partial f}{\\partial \\beta_k}$$ как $$\\Delta \\beta_k = -\\eta \\frac{\\partial E}{\\partial \\beta_k}$$. \n",
    "\n",
    "Тогда после преобразований выше:\n",
    "\n",
    "$$\\Delta \\beta_k = \\eta (y - f(X)) h_k(X)$$\n",
    "\n",
    "Так мы определили градиентный спуск для бет. Остаются веса. Как обновить веса $w_{kj}$, которые спрятаны внутри нелинейности $g$.\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{kj}} = \\underbrace{\\frac{\\partial E}{\\partial f} \\cdot \\frac{\\partial f}{\\partial h_k}}_{\\text{ошибка с выхода}} \\cdot \\underbrace{\\frac{\\partial h_k}{\\partial z_k}}_{\\text{производная активации}} \\cdot \\underbrace{\\frac{\\partial z_k}{\\partial w_{kj}}}_{\\text{вход}}$$\n",
    "\n",
    "Где $z_k = w_{k0} + \\sum w_{kj} X_j$. Разберем компоненты:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial f} = -(y - f(X))$;\n",
    "\n",
    "$\\frac{\\partial f}{\\partial h_k} = \\beta_k$ (ошибка распределяется пропорционально весу связи);\n",
    "\n",
    "$\\frac{\\partial h_k}{\\partial z_k} = g'(z_k)$ — производная функции активации (ReLU или сигмоиды);\n",
    "\n",
    "$\\frac{\\partial z_k}{\\partial w_{kj}} = X_j$.\n",
    "\n",
    "Будем брать производные по элементам.\n",
    "\n",
    "($\\frac{\\partial E}{\\partial f}$): Производная квадратичной ошибки по выходу:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial f} \\left[ \\frac{1}{2}(y - f)^2 \\right] = -(y - f)$$\n",
    "\n",
    "($\\frac{\\partial f}{\\partial h_k}$): Насколько выход $f$ изменится при изменении $h_k$. Поскольку $f = \\dots + \\beta_k h_k + \\dots$, производная равна весу связи:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial h_k} = \\beta_k$$\n",
    "\n",
    "($\\frac{\\partial h_k}{\\partial z_k}$): Прохождение сигнала через нелинейную функцию активации $g$:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial z_k} [g(z_k)] = g'(z_k)$$\n",
    "\n",
    "($\\frac{\\partial z_k}{\\partial w_{kj}}$): Влияние конкретного веса на сумму входов $z_k = \\dots + w_{kj} X_j + \\dots$:\n",
    "\n",
    "$$\\frac{\\partial z_k}{\\partial w_{kj}} = X_j$$\n",
    "\n",
    "Теперь подставляем все звенья в формулу шага $\\Delta w_{kj} = -\\eta (\\dots)$:\n",
    "\n",
    "$$\\Delta w_{kj} = -\\eta \\cdot \\left[ \\underbrace{-(y - f)}_{\\text{Звено 1}} \\cdot \\underbrace{\\beta_k}_{\\text{Звено 2}} \\cdot \\underbrace{g'(z_k)}_{\\text{Звено 3}} \\cdot \\underbrace{X_j}_{\\text{Звено 4}} \\right]$$\n",
    "\n",
    "Минус перед $\\eta$ и минус из производной ошибки сокращаются, давая итоговое выражение:\n",
    "\n",
    "$$\\Delta w_{kj} = \\eta \\cdot \\underbrace{(y - f(X)) \\beta_k}_{\\text{проброшенная ошибка}} \\cdot \\underbrace{g'(z_k)}_{\\text{фильтр}} \\cdot \\underbrace{X_j}_{\\text{вход}}$$\n",
    "\n",
    "Если в качестве $g$ вы используете ReLU, то её производная $g'(z)$ крайне проста, что делает вычисления очень быстрыми:\n",
    "\n",
    "$$g'(z) = \\begin{cases} 1, & z > 0 \\\\ 0, & z \\le 0 \\end{cases}$$\n",
    "\n",
    "Это означает, что если нейрон был \"выключен\" (выдал 0 на прямом проходе), то градиент через него не течет ($\\Delta w = 0$), и он не обучается на этом шаге.\n",
    "\n",
    "Процесс обновления весов после вычисления градиента (шага $\\Delta w$) выглядит так:\n",
    "\n",
    "$$w_{kj}^{(new)} = w_{kj}^{(old)} + \\Delta w_{kj}$$\n",
    "\n",
    "Предположим, мы предсказываем рейтинг книги (от 0 до 5).\n",
    "\n",
    "Исходные данные и состояние сети:\n",
    "\n",
    "Вход ($X_j$): Количество страниц = $300$.\n",
    "\n",
    "Истинный рейтинг ($y$): $5.0$.\n",
    "\n",
    "Текущее предсказание сети ($f(X)$): $3.0$.\n",
    "\n",
    "Вес внешнего слоя ($\\beta_k$): $0.5$.\n",
    "\n",
    "Текущий вес внутреннего слоя ($w_{kj}$): $0.01$.\n",
    "\n",
    "Скорость обучения ($\\eta$): $0.0001$.\n",
    "\n",
    "Производная активации ($g'$): Пусть нейрон активен (ReLU), тогда $g' = 1$.\n",
    "\n",
    "Вычисляем компоненты $\\Delta w_{kj}$:\n",
    "\n",
    "Ошибка: $(y - f(X)) = 5.0 - 3.0 = 2.0$. (Сеть занизила рейтинг).\n",
    "\n",
    "Проброшенная ошибка: $2.0 \\cdot 0.5 = 1.0$.\n",
    "\n",
    "Итоговый шаг ($\\Delta w_{kj}$):\n",
    "\n",
    "$$\\Delta w_{kj} = 0.0001 \\cdot 1.0 \\cdot 1 \\cdot 300 = 0.03$$\n",
    "\n",
    "Обновление веса:\n",
    "\n",
    "$$w_{kj}^{(new)} = 0.01 + 0.03 = 0.04$$\n",
    "\n",
    "Вес увеличился. В следующий раз при входе «300 страниц» этот нейрон выдаст более высокое значение, и итоговое предсказание $f(X)$ станет ближе к $5.0$.\n",
    "\n",
    "Остается сделать несколько замечаний перед тем, как мы попробуем запустить нейронную сеть. \n",
    "\n",
    "1. Персептрон очень напоминает логистическую регрессию. Это хорошо видно на следующем рисунке\n",
    "\n",
    "<img src=\"./images/tensor3.png\" alt=\"Тензор\" title=\"Тензор\" width=\"300\" />\n",
    "\n",
    "2. Этапы применения нейронной сети следующие:\n",
    "\n",
    "- сначала задаем обработку данных и слои нейронной сети;\n",
    "\n",
    "- затем компилируем, задаем способ оптимизации, метрики;\n",
    "\n",
    "- обучаем;\n",
    "\n",
    "- делаем предсказания.\n",
    "\n",
    "3. Важнейшие понятия машинного обучения и нейронных сетей.\n",
    "\n",
    "- Инициализация модели, параметров. Это задание функции, начальных значений весов для независимыех переменных, смещения. Как пример, задание весов и смещения в линейной регрессии. В случае, если используется вероятностный подход (например, логистическая регрессия), формула линейной регрессии дополнительно \"помещается\" в функцию, которая расчитывает вероятность, например softmax.\n",
    "\n",
    "- Функция активации. В каждом слое применяется линейное преобразование, например функция линейной регрессии. Эта функция \"оборачивается\" нелинейной функцией. Например, функцией активации, в которую \"помещается\" линейная функция, будет ReLU, сигмоидная, тангенциальная функция. Задача указанных функций в том, чтобы на них можно было с большей эффективностью вычислять минимумы.\n",
    "\n",
    "- Функция потерь. Это функция, которая измеряет разницу между фактическим значением зависимой переменной и значением зависимой переменной, которое было спрогнозировано моделью машинного обучения. В обычной линейной регрессии функцией потерь может быть среднеквадратичная ошибка, в логистической регрессии функция потерь будет основана на методе максимального правдоподобия. Здесь может быть применена регуляризаця, когда к значению потери прибавляется дополнительный (штрафной) член, а задача минимизации значения потери становится задачей минимизации потери плюс штрафа.\n",
    "\n",
    "- Методы оптимизации (градиентный спуск и т.д.). Это способ нахождения минимума функции потерь за счет обновления параметров модели.\n",
    "\n",
    "- Обучение модели. Это процесс, который начинается с инициализации параметров, продолжается прогнозированием значения зависимой переменной, расчетом функции потерь, а затем применением выбранного метода оптимизации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf43ce6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_rating</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>text_reviews_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.59</td>\n",
       "      <td>501</td>\n",
       "      <td>4597666</td>\n",
       "      <td>94265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.27</td>\n",
       "      <td>366</td>\n",
       "      <td>2530894</td>\n",
       "      <td>32871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.80</td>\n",
       "      <td>277</td>\n",
       "      <td>2457092</td>\n",
       "      <td>43499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   average_rating  num_pages  ratings_count  text_reviews_count\n",
       "0            3.59        501        4597666               94265\n",
       "1            4.27        366        2530894               32871\n",
       "2            3.80        277        2457092               43499"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(os.getcwd() + '\\\\gd_clean_data.csv') \n",
    "db = data.copy()\n",
    "db = db.drop(['title', 'language_code', 'authors', 'editions_count', 'year', 'quarter'], axis=1)\n",
    "db.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7214e813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаю обучение нейросети...\n",
      "Iteration 1, loss = 6.34793415\n",
      "Validation score: -82.911912\n",
      "Iteration 2, loss = 2.18927710\n",
      "Validation score: -17.264919\n",
      "Iteration 3, loss = 0.45553886\n",
      "Validation score: -6.230620\n",
      "Iteration 4, loss = 0.25713662\n",
      "Validation score: -3.882337\n",
      "Iteration 5, loss = 0.17430934\n",
      "Validation score: -2.577289\n",
      "Iteration 6, loss = 0.12091533\n",
      "Validation score: -1.421497\n",
      "Iteration 7, loss = 0.08811845\n",
      "Validation score: -0.870228\n",
      "Iteration 8, loss = 0.07004017\n",
      "Validation score: -0.557846\n",
      "Iteration 9, loss = 0.06123368\n",
      "Validation score: -0.342194\n",
      "Iteration 10, loss = 0.05424702\n",
      "Validation score: -0.193313\n",
      "Iteration 11, loss = 0.04998800\n",
      "Validation score: -0.122409\n",
      "Iteration 12, loss = 0.04700173\n",
      "Validation score: -0.069970\n",
      "Iteration 13, loss = 0.04535985\n",
      "Validation score: -0.031844\n",
      "Iteration 14, loss = 0.04414546\n",
      "Validation score: -0.018655\n",
      "Iteration 15, loss = 0.04351218\n",
      "Validation score: -0.003254\n",
      "Iteration 16, loss = 0.04362885\n",
      "Validation score: 0.005002\n",
      "Iteration 17, loss = 0.04281246\n",
      "Validation score: 0.009575\n",
      "Iteration 18, loss = 0.04265509\n",
      "Validation score: 0.010487\n",
      "Iteration 19, loss = 0.04236653\n",
      "Validation score: 0.013313\n",
      "Iteration 20, loss = 0.04218366\n",
      "Validation score: 0.024484\n",
      "Iteration 21, loss = 0.04201386\n",
      "Validation score: 0.026293\n",
      "Iteration 22, loss = 0.04194641\n",
      "Validation score: 0.020423\n",
      "Iteration 23, loss = 0.04187668\n",
      "Validation score: 0.028854\n",
      "Iteration 24, loss = 0.04177941\n",
      "Validation score: 0.032564\n",
      "Iteration 25, loss = 0.04172084\n",
      "Validation score: 0.036400\n",
      "Iteration 26, loss = 0.04173059\n",
      "Validation score: 0.033503\n",
      "Iteration 27, loss = 0.04176412\n",
      "Validation score: 0.026313\n",
      "Iteration 28, loss = 0.04167117\n",
      "Validation score: 0.035928\n",
      "Iteration 29, loss = 0.04158031\n",
      "Validation score: 0.033808\n",
      "Iteration 30, loss = 0.04157765\n",
      "Validation score: 0.039921\n",
      "Iteration 31, loss = 0.04150348\n",
      "Validation score: 0.040876\n",
      "Iteration 32, loss = 0.04152261\n",
      "Validation score: 0.041933\n",
      "Iteration 33, loss = 0.04141808\n",
      "Validation score: 0.041783\n",
      "Iteration 34, loss = 0.04138040\n",
      "Validation score: 0.035074\n",
      "Iteration 35, loss = 0.04134891\n",
      "Validation score: 0.041636\n",
      "Iteration 36, loss = 0.04130035\n",
      "Validation score: 0.047472\n",
      "Iteration 37, loss = 0.04134765\n",
      "Validation score: 0.042630\n",
      "Iteration 38, loss = 0.04126024\n",
      "Validation score: 0.040666\n",
      "Iteration 39, loss = 0.04131456\n",
      "Validation score: 0.043526\n",
      "Iteration 40, loss = 0.04117772\n",
      "Validation score: 0.048249\n",
      "Iteration 41, loss = 0.04122749\n",
      "Validation score: 0.048498\n",
      "Iteration 42, loss = 0.04115003\n",
      "Validation score: 0.050590\n",
      "Iteration 43, loss = 0.04114860\n",
      "Validation score: 0.048187\n",
      "Iteration 44, loss = 0.04135967\n",
      "Validation score: 0.050522\n",
      "Iteration 45, loss = 0.04129159\n",
      "Validation score: 0.041772\n",
      "Iteration 46, loss = 0.04131494\n",
      "Validation score: 0.053594\n",
      "Iteration 47, loss = 0.04111261\n",
      "Validation score: 0.041932\n",
      "Iteration 48, loss = 0.04109772\n",
      "Validation score: 0.050422\n",
      "Iteration 49, loss = 0.04096849\n",
      "Validation score: 0.054776\n",
      "Iteration 50, loss = 0.04101131\n",
      "Validation score: 0.047159\n",
      "Iteration 51, loss = 0.04095951\n",
      "Validation score: 0.050199\n",
      "Iteration 52, loss = 0.04104322\n",
      "Validation score: 0.052933\n",
      "Iteration 53, loss = 0.04101787\n",
      "Validation score: 0.044222\n",
      "Iteration 54, loss = 0.04093039\n",
      "Validation score: 0.054012\n",
      "Iteration 55, loss = 0.04099145\n",
      "Validation score: 0.054540\n",
      "Iteration 56, loss = 0.04109781\n",
      "Validation score: 0.046959\n",
      "Iteration 57, loss = 0.04103541\n",
      "Validation score: 0.051891\n",
      "Iteration 58, loss = 0.04105649\n",
      "Validation score: 0.045910\n",
      "Iteration 59, loss = 0.04115083\n",
      "Validation score: 0.040350\n",
      "Iteration 60, loss = 0.04096884\n",
      "Validation score: 0.043114\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "------------------------------\n",
      "Средняя абсолютная ошибка (MAE): 0.2219\n",
      "Среднеквадратичная ошибка (MSE): 0.0914\n",
      "R2 Score (Коэффициент детерминации): 0.0589\n"
     ]
    }
   ],
   "source": [
    "# 1. Подготовка данных (используем ваш db)\n",
    "X = db.drop('average_rating', axis=1)\n",
    "y = db['average_rating']\n",
    "\n",
    "# 2. Разделение на обучение и тест\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Стандартизация (обязательна для нейросетей)\n",
    "# Приводим данные к нулевому среднему и единичной дисперсии\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Создание и обучение модели\n",
    "# hidden_layer_sizes=(64, 32): два скрытых слоя с 64 и 32 нейронами\n",
    "# solver='adam': стохастический градиентный оптимизатор\n",
    "# early_stopping=True: остановка обучения, если ошибка перестала падать (защита от переобучения)\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    verbose=True  # Вывод хода обучения\n",
    ")\n",
    "\n",
    "print(\"Начинаю обучение нейросети...\")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 5. Предсказание и оценка\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Средняя абсолютная ошибка (MAE): {mae:.4f}\")\n",
    "print(f\"Среднеквадратичная ошибка (MSE): {mse:.4f}\")\n",
    "print(f\"R2 Score (Коэффициент детерминации): {model.score(X_test_scaled, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda070ce",
   "metadata": {},
   "source": [
    "Результаты слабые, но цели обучить модель на \"отлично\" мы и не ставили. Наша цель - показать, как работает нейронная сеть. А это мы сделали."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
